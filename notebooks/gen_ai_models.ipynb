{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging for chain-of-thought outputs\n",
    "logging.basicConfig(\n",
    "    filename='../logs/reasoning.log',  # Adjust path if running from notebook root\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../config/models_config.yaml'\n",
    "try:\n",
    "    import yaml\n",
    "    with open(config_path, 'r') as file:\n",
    "        model_catalog = yaml.safe_load(file)\n",
    "except Exception as e:\n",
    "    # Fallback: define a default catalog if config file is not available\n",
    "    model_catalog = {\n",
    "        \"openai\": [\"gpt-4o\", \"gpt-3.5-turbo\"],\n",
    "        \"gemini\": [],\n",
    "        \"llama\": [],\n",
    "        \"claude\": [],\n",
    "        \"deepseek\": []\n",
    "    }\n",
    "print(\"Available models:\", model_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your .env file has: OPENAI_API_KEY=your_key_here\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Please set your OPENAI_API_KEY in the .env file.\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def query_openai(prompt, model=\"gpt-4o\", log_verbose=True):\n",
    "    \"\"\"\n",
    "    Query the OpenAI model and optionally log verbose chain-of-thought output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            # You might add parameters that enable chain-of-thought if available\n",
    "        )\n",
    "        # Extract final output\n",
    "        final_output = response.choices[0].message['content']\n",
    "        \n",
    "        # If verbose chain-of-thought info is available (this may vary by model/implementation)\n",
    "        verbose_info = response.get(\"verbose\", \"No verbose chain-of-thought provided.\")\n",
    "        if log_verbose:\n",
    "            logging.info(f\"Prompt: {prompt}\\nVerbose Output: {verbose_info}\")\n",
    "        \n",
    "        return final_output\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error querying OpenAI: \" + str(err))\n",
    "        return f\"Error: {err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain why Paris is called the city of lights.\"\n",
    "output = query_openai(prompt, model=\"gpt-4o\")\n",
    "print(\"Model Output:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
